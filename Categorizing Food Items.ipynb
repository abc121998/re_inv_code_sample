{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f852bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d1055",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text=text.lower()\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text=text.strip()\n",
    "    text += \" \"\n",
    "    return text\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "\n",
    "def tokenizeIngredients(text):\n",
    "    snowball = SnowballStemmer(language='english')\n",
    "    token_words=word_tokenize(text)\n",
    "    stem_sentence= \"\"\n",
    "    for word in token_words:\n",
    "        stem_sentence += snowball.stem(word) + \" \"\n",
    "    return stem_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e66265",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_items = pd.read_csv('test.csv', delimiter=\",\")\n",
    "columns = ['eanCode','name','categories','ingredients','nutrient_data','url','swap_cat']\n",
    "food_items = food_items.loc[:,columns]\n",
    "food_items = food_items.fillna('NA')\n",
    "food_items = food_items.sort_values('ingredients',key=lambda x: x.str.len())\n",
    "\n",
    "start = \"text\"\n",
    "ingred_list = []\n",
    "\n",
    "for i in range(food_items['ingredients'].size):\n",
    "    ingredients = ''\n",
    "    for ingred in food_items['ingredients'][i].split(','):\n",
    "        ingredients += preprocess_text(ingred) + ' '\n",
    "    ingred_list.append(ingredients )\n",
    "food_items['clean_ingredients'] = ingred_list\n",
    "\n",
    "food_items_docs = pd.DataFrame(food_items['name'] + \"; \" + food_items['categories']  + \"; \" + food_items['clean_ingredients'])\n",
    "food_items_docs = food_items_docs.iloc[:,0].astype('U')\n",
    "food_items_text = []\n",
    "for item in food_items_docs:\n",
    "    food_items_text.append(tokenizeIngredients(preprocess_text(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4759f4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#cross valid\n",
    "X = food_items_text\n",
    "y = food_items['swap_cat']\n",
    "count_vect_cv = CountVectorizer(stop_words=[w for w in stopwords.words('english')])\n",
    "X_counts = count_vect_cv.fit_transform(X)\n",
    "tf_transformer_cv = TfidfTransformer(use_idf=False).fit(X_counts)\n",
    "X_tfidf = tf_transformer_cv.transform(X_counts)\n",
    "word_df = pd.DataFrame(count_vect_cv.vocabulary_,index=[0]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37656afc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC(C=1.0,max_iter=10000,random_state=42)\n",
    "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=3, tol=None)\n",
    "svc = svm.SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cde069",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_pipeline=Pipeline([\n",
    "    ('vect',  CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',svc)\n",
    "])\n",
    "parameters = {\n",
    "    # vectorizer hyper-parameters\n",
    "    'vect__ngram_range': [(1,1),(1,3)],\n",
    "    'vect__max_df': [0.4],\n",
    "    'vect__max_features': [300,5000],\n",
    "    'vect__stop_words': [{'english'},['food','id','base', 'and']],\n",
    "    'clf__kernel': ['linear', 'sigmoid'],\n",
    "    'clf__C':[0.5,1,1.5,1.8],\n",
    "    'clf__gamma':['auto','scale'],\n",
    "    'clf__tol':[0.1,0.01,0.001],\n",
    "}\n",
    "\n",
    "# create grid search object, and use the pipeline as an estimator\n",
    "svcgrid_search = GridSearchCV(svc_pipeline, parameters, n_jobs=-1)\n",
    "\n",
    "# fit the grid search on the training data\n",
    "svcgrid_search.fit(X,y)\n",
    "\n",
    "# get the list of optimal parameters\n",
    "df = pd.DataFrame(svcgrid_search.cv_results_)\n",
    "svcgrid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674deee3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf54269",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lin_svc_pipeline = Pipeline([\n",
    "    ('vect',  CountVectorizer(stop_words={'english'})),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', lin_clf)\n",
    "])\n",
    "parameters = {\n",
    "    # vectorizer hyper-parameters\n",
    "    'vect__stop_words': [{'english'},['food','id','base', 'and', 'beverage']],\n",
    "    'vect__ngram_range': [(1,1),(1, 3)],\n",
    "    'vect__max_df': [0.2,0.4],\n",
    "    'vect__max_features': [300,700,1000],\n",
    "    # classifiers\n",
    "    'clf__C':[0.5,1,1.5,1.8],\n",
    "    'clf__tol':[0.1,0.01,0.001],\n",
    "    'clf__loss':['hinge', 'squared_hinge'],\n",
    "    'clf__max_iter':[1000,5000,7000]\n",
    "\n",
    "}\n",
    "\n",
    "# create grid search object, and use the pipeline as an estimator\n",
    "grid_search = GridSearchCV(lin_svc_pipeline, parameters, n_jobs=-1)\n",
    "\n",
    "# fit the grid search on the training data\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "# get the list of optimal parameters\n",
    "df = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463f535",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d976c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sgd_pipeline = Pipeline([\n",
    "    ('vect',  CountVectorizer(stop_words={'english'})),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', sgd_clf)\n",
    "])\n",
    "parameters = {\n",
    "    # vectorizer hyper-parameters\n",
    "    'vect__ngram_range': [(1,1),(1, 3)],\n",
    "    'vect__max_df': [0.2,0.4],\n",
    "    'vect__max_features': [300,5000],\n",
    "    'vect__stop_words': [{'english'},['food','id','base', 'and', 'beverage']],\n",
    "    # classifier hyper-parameers\n",
    "    'clf__loss':['hinge','modified_huber','perceptron','squared_hinge'],\n",
    "    'clf__alpha':[0.0001,0.001,0.01],\n",
    "    'clf__tol':[0.00001,0.0001],\n",
    "    'clf__max_iter':[5000]\n",
    "\n",
    "}\n",
    "\n",
    "# create grid search object, and use the pipeline as an estimator\n",
    "grid_search = GridSearchCV(sgd_pipeline, parameters, n_jobs=-1,error_score='raise')\n",
    "\n",
    "# fit the grid search on the training data\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "# get the list of optimal parameters\n",
    "df = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09159f77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e71cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words={'english'},ngram_range=(1,3),max_features=300,max_df= 0.4)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='modified_huber', alpha=0.01, random_state=42, tol=1e-05,max_iter=5000)),\n",
    "])\n",
    "\n",
    "text_clf.fit(X,y)\n",
    "scores = cross_val_score(text_clf, X, y, cv= StratifiedKFold(n_splits=5))\n",
    "print(scores.mean())\n",
    "\n",
    "#filename = 'item_category_svc_model.sav'\n",
    "#pickle.dump(text_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fda8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y,text_clf.predict(X))\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d', xticklabels=food_items['swap_cat'].unique(),yticklabels=food_items['swap_cat'].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c10685",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}